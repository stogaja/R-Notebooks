---
title: "Unsupervised Learning with R"
author: "STEPHEN ODHIAMBO OGAJA"
date: '2022-06-04'
output:
  pdf_document: default
  html_document:
    df_print: paged
---

# TARGETED CRYPTOGRAPHY ADVERTISING

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Defining the Question

### a) Specifying the Question

Which individuals are more likely to click on the cryptography course adverts?

### b) Defining the metric of success

The project will be considered a success when we can identify which individuals will click on the advert and factors that affect ad clicks.

### c) Understanding the context

A Kenyan entrepreneur has created an online cryptography course and would want to advertise it on her blog. She currently targets audiences originating from various countries. In the past, she ran ads to advertise a related course on the same blog and collected data in the process. She would now like to employ your services as a Data Science Consultant to help her identify which individuals are most likely to click on her ads.

### d) Recording the experimental design

    1. Data sourcing/loading
    2. Data Understanding
    3. Data Relevance
    4. External Dataset Validation
    5. Data Preparation
    6. Univariate Analysis
    7. Bivariate Analysis
    8. Multivariate Analysis
    9. Implementing the solution
    10. Challenging the solution
    11. Conclusion
    12. Follow up questions

### e) Data Relevance

For relevant data, the data should be able to provide meaningful insights that can be used to isolate users who are most likely to click in the ads.


## 2. Data Understanding

#### Loading Libraries
```{r load-packages, include=FALSE}
# let's load the libraries we need
library(dplyr)
library(data.table)
library (plyr)
library(ggplot2)
library(moments)
library(ggcorrplot)
library(magrittr)
library(knitr)
library(rpart)
library(caret)
```

### a) Reading the data

#### Let's read our dataset
```{r}
# let's import our dataset
adverts <- read.csv("advertising.csv")
```

### b) Checking the Data

#### Top records
```{r}
# let's preview the top of our dataset
head(adverts)
```

#### Bottom records
```{r}
# let's preview the last 6 records of our dataset
tail(adverts)
```

#### The shape of the dataset
```{r}
# let's see the number of rows and columns in our dataset
cat("The dataset has ", nrow(adverts), "rows and ", ncol(adverts), "columns")
```

### c) data types of the variables
```{r}
str(adverts)
```
R stores the dataframe and views the variables as lists so to see the the various data types of this list we use the str function.

#### let's check for duplicates in the dataframe
```{r}
duplicates <- adverts[duplicated(adverts), ]
duplicates
```
The dataframe does not contain duplicate values.

#### let's check for missing data in each column
```{r}
colSums(is.na(adverts))
```
The dataset's columns does not have missing data.

#### let's check for outliers in the dataset
#### selecting only numeric columns
```{r}
num_cols <- adverts[,unlist(lapply(adverts, is.numeric))]
head(num_cols)
```
6 columns are numerical in nature

#### let's check for outliers in the numerical columns using BOXPLOT
```{r}
boxplot(num_cols, notch = TRUE)
```
The Area.Income variable has outliers which will be imputed.

#### let's see the values which are outliers in the Area.Income variable
```{r}
boxplot.stats(adverts$Area.Income)$out
```



#### let's check for outliers using Z-SCORES

#### The z-score indicates the number of standard deviations a given value deviates from the mean.
```{r}
z_scores <- as.data.frame(sapply(num_cols, function(num_cols) (abs(num_cols-mean(num_cols))/sd(num_cols))))
head(z_scores)
```
We will drop values with a Z-Score of more than 3 or -3. They are the outliers

#### Removing the outliers
```{r}
no_outliers <- z_scores[!rowSums(z_scores>3), ]
head(no_outliers)
```

#### let's check the number of observations after removing outliers
```{r}
dim(num_cols)
dim(no_outliers)
```
We removed 2 observations.

#### let's check for outliers in the new dataframe after removing them
```{r}
boxplot(no_outliers)
```
There are still outliers so we will use interquantile range method to remove outliers

#### checking and removing outliers using IQR

#### The Area.Income column had outliers so we focus on it
```{r}
income.IQR <- 65471-47032
income.IQR <-IQR(adverts$`Area.Income`)
income.IQR
```

#### let's save the dataframe without outliers into a new dataframe by assigning it to a variable
```{r}
adverts_2 <- subset(adverts, adverts$`Area.Income`> (47032 - 1.5*income.IQR) & adverts$`Area.Income`<(65471 + 1.5*income.IQR))
```

#### let's see the shape of the new dataframe
```{r}
dim(adverts_2)
```
We have lost 9 observations that included the outliers. We proceed with analysis.




## 3. Exploratory Data Analysis

### {UNIVARIATE ANALYSIS}

#### let's get the mean of the numerical columns
```{r}
summary(num_cols)
```
The summary shows:
  -The minimum value for each numerical variable.
  -The first quantile for each numerical variable
  -The median value for all numeric variables across the dataframe.
  -The mean value for all numeric variables.
  -The third quantile.
  -The maximum value for all numerical columns.

#### let's get the variance for the numeric variables
```{r}
variance <- var(num_cols)
variance
```
variance is a measure of how far the set of data points per column is spread out from their mean eg. those of the area income seem to be far spread out from their mean when compared to that of the age column.

#### let's get the standard deviation of the numeric variables

#### let's create a function to get the standard deviations
```{r}
sd.function <- function(column) {
  standard.deviations <- sd(column)
  print(standard.deviations)
} 
```

#### standard deviation for daily time spent on site
```{r}
sd.function(adverts_2$Daily.Time.Spent.on.Site)
```

#### standard deviation for Age
```{r}
sd.function(adverts_2$Age)
```

#### standard deviation for Area.Income
```{r}
sd.function(adverts_2$Area.Income)
```

#### standard deviation for Daily.Internet.Usage
```{r}
sd.function(adverts_2$Daily.Internet.Usage)
```
Where a low standard deviation indicates that values are closer to the mean a high one indicates the standard deviation is far from the mean e.g the age column standard deviation of 8.8 displays that its values are closer to their mean than that of the Area income column whose value is 12961

#### let's get the skewness of the numerical column
```{r}
library(moments)
skewness(num_cols)
```
The skewness of the Age variable being positive indicates that its distribution has a longer right tail than left tail while the rest of the columns' left tails.




### {BIVARIATE ANALYSIS}

#### let's get the covariance of the numeric variables
```{r}
cov(num_cols)
```
The age variable is the only column with a positive covariance with the ad click variable, the rest have negative covariances.

#### let's get the correlation coefficient
```{r}
cor(num_cols)
```
The variables have a negative correlation with the target variable apart from the age variable which has a positive correlation. Let's see that in the correlogram below

#### let's see the corrplot of the numeric variables
```{r}
library(corrplot)

corr_ <- cor(num_cols)

corrplot(corr_, method = 'color')
```

## 4. Modelling

We will do logistic regression as the study requires a classification solution

```{r}
# let's split the data into training and test splits
set.seed(100)
# let's select only columns that are relevant to modeling
cols = c('Daily.Time.Spent.on.Site', 'Age', 'Area.Income', 'Daily.Internet.Usage', 'Male', 'Clicked.on.Ad')
advertising = select(adverts_2, all_of(cols))

train_rows = createDataPartition(advertising$Clicked.on.Ad, p=0.8, list=FALSE)

# training  dataset
train = advertising[train_rows,]

# test dataset
test = advertising[-train_rows,]

# lets create the  X and y variables
X = train
y = train$Clicked.on.Ad
```

### a) Decsion Trees

```{r}
# let's install tree package
#install.packages("tree")
```



```{r}
# let's train the model
require(tree)
model_ <- tree(Clicked.on.Ad ~.,
               data = train,
               method = "ranger")
model_
```

```{r}
# let's make predictions
y_pred_ <- predict(model_)
head(y_pred_)
```

```{r}
# let's find the confusion matrix and the accuracy scores of the model
confusion_Matrix <- table(Actual = train$Clicked.on.Ad, predicted = y_pred_ > .5)
confusion_Matrix
```

```{r}
# let's find the accuracy
(confusion_Matrix[[1,1]] + confusion_Matrix[[2,2]])/sum(confusion_Matrix)
```


The Decision Trees model gives an accuracy of 96%


### b) Logistic Regression

#### We will use the x and y set above
```{r}
# let's now train our model
model <- glm(Clicked.on.Ad ~ .,
             data = train,
             family = "binomial")

# let's see a summary of our model
summary(model)
```
```{r}
# let's make predictions
y_pred <- predict(model)

# let's run our test through the model
b <- predict(model, test, type = "response")
head(b)

b <- predict(model, train, type = "response")
head(b)
```

```{r}
# let's validate the model
matrix <- table(Actual_Value = train$Clicked.on.Ad, Predicted_Value = b > .5)
matrix
```

```{r}
# let's get our accuracy score
(matrix[[1,1]] + matrix[[2,2]])/sum(matrix)
```

The model attains an accuracy of 97% on logistic regression


## 5. Conclusion
The logistic regression model having given an accuracy score of 97% is better than the SVM model performed first.

## 6. RECOMMENDATIONS

 a. The entrepreneur should focus on the older population as the correlation between age and advert clicks is slightly positive indicating that as age increases the more likely the clicks are made.

b. The entreoreneur should focus on regions with bigger area coverage as those with a smaller area since the correlation between area income and advert clicks is negatively weak one indicating that as area income decreases the more likely the clicks are made and vice versa.

c. She should focus on the regions with low daily internet usage because the correlation between the daily internet usage and clicks on ads is negative indicating that as internet use decreases the more likely the clicks will be made.

## 7. Follow up Questions

### a) Did we have the right data?

Yes, the data we were provided with was correct and it fit the scope of the analysis

### b) Did we need any other data to answer questions?

Yes, availability of more data on customer behavior would have given us more insight and perhaps a more accurate model

### c) Did we have the right Question?

Yes we did, the main question married well with the data we were provided with.